# -*- coding: utf-8 -*-
"""Copy of gpt-oss-(20B)-Fine-tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Au6fNuILy9n_eZjQIKGiEZfY7iqR4Sb

### Installation
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os, importlib.util
# !pip install --upgrade -qqq uv
# if importlib.util.find_spec("torch") is None or "COLAB_" in "".join(os.environ.keys()):
#     try: import numpy, PIL; get_numpy = f"numpy=={numpy.__version__}"; get_pil = f"pillow=={PIL.__version__}"
#     except: get_numpy = "numpy"; get_pil = "pillow"
#     !uv pip install -qqq \
#         "torch>=2.8.0" "triton>=3.4.0" {get_numpy} {get_pil} torchvision bitsandbytes "transformers==4.56.2" \
#         "unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo" \
#         "unsloth[base] @ git+https://github.com/unslothai/unsloth" \
#         git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels
# elif importlib.util.find_spec("unsloth") is None:
#     !uv pip install -qqq unsloth
# !uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo

"""### Unsloth

✅ **GPU REQUIREMENTS:**
Qwen3-8B with 4-bit quantization uses ~5-6GB VRAM

**Works on:** T4 (14.7GB), V100, A100, or any GPU with 8GB+ VRAM
- No special GPU required!

**Memory breakdown:**
- Model weights (4-bit): ~4 GB
- Activations (1024 seq): ~1-2 GB
- LoRA adapters: ~500 MB
- Total: ~5-6 GB

**Model Details:**
- Qwen3-8B: General-purpose 8B model with strong code capabilities
- Supports 32k context length (using 1024 for efficiency)
- Has thinking mode for enhanced reasoning
- Uses ChatML format (not Harmony)
"""

from unsloth import FastLanguageModel
import torch
max_seq_length = 1024  # Qwen3 supports up to 32k, using 1024 for speed
dtype = None

# Try Unsloth's optimized version first, fallback to base if needed
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Qwen3-8B-bnb-4bit",  # 8B model, fits T4 GPU easily
    dtype = dtype, # None for auto detection
    max_seq_length = max_seq_length,
    load_in_4bit = True,  # 4 bit quantization to reduce memory
    full_finetuning = False,
    # token = "hf_...", # use one if using gated models
)

"""We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters."""

model = FastLanguageModel.get_peft_model(
    model,
    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

from transformers import TextStreamer

# Test code completion before training - Python example
messages = [
    {"role": "system", "content": "You are an expert code completion assistant. Complete the code at the <FILL_HERE> marker with syntactically correct Python code that fits naturally with the surrounding context."},
    {"role": "user", "content": """### Complete the following Python code:

```python
def calculate_average(numbers):
    if not numbers:
        return 0
    <FILL_HERE>
```"""},
]
inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
    return_dict = True,
).to("cuda")

_ = model.generate(**inputs, max_new_tokens = 128, streamer = TextStreamer(tokenizer))

from transformers import TextStreamer

# Test code completion - JavaScript example
messages = [
    {"role": "system", "content": "You are an expert code completion assistant. Complete the code at the <FILL_HERE> marker with syntactically correct JavaScript code that fits naturally with the surrounding context."},
    {"role": "user", "content": """### Complete the following JavaScript code:

```javascript
function fetchUserData(userId) {
    return fetch(`/api/users/${userId}`)
        .then(response => response.json())
        <FILL_HERE>
```"""},
]
inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
    return_dict = True,
).to("cuda")

_ = model.generate(**inputs, max_new_tokens = 128, streamer = TextStreamer(tokenizer))

from transformers import TextStreamer

# Test code completion - C++ example
messages = [
    {"role": "system", "content": "You are an expert code completion assistant. Complete the code at the <FILL_HERE> marker with syntactically correct C++ code that fits naturally with the surrounding context."},
    {"role": "user", "content": """### Complete the following C++ code:

```cpp
class Vector3D {
public:
    float x, y, z;
    <FILL_HERE>
};
```"""},
]
inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
    return_dict = True,
).to("cuda")

_ = model.generate(**inputs, max_new_tokens = 128, streamer = TextStreamer(tokenizer))

"""<a name="Data"></a>
### Data Prep

The `sourcegraph/context-aware-fim-code-completions` dataset will be utilized for this code completion fine-tuning task. This dataset, available on Hugging Face, contains 13.5k fill-in-the-middle (FIM) code completion examples with context-aware snippets retrieved from repositories. Each example includes:
- Code prefix and suffix (the surrounding code)
- The middle section to be predicted
- Relevant code context items from the repository (with file paths and relevance scores)
- Programming language identifier

The purpose of using this dataset is to enable the model to learn context-aware code completion capabilities across multiple programming languages, using both local code context (prefix/suffix) and relevant snippets from the broader codebase.
"""

def formatting_prompts_func(examples):
    """Convert FIM (Fill-in-the-Middle) code completion data to Harmony message format"""
    texts = []
    skipped = 0

    for i in range(len(examples["prefix"])):
        lang = examples["lang"][i] if "lang" in examples else "Unknown"
        prefix = examples["prefix"][i]
        suffix = examples["suffix"][i]
        middle = examples["middle"][i]
        context_items = examples.get("context_items", [None] * len(examples["prefix"]))[i]

        # Validation: Skip examples with empty or invalid middle completion
        if not middle or not isinstance(middle, str) or len(middle.strip()) == 0:
            skipped += 1
            # Add empty text to maintain index alignment
            texts.append("")
            continue

        # Validation: Skip if prefix and suffix are both empty (no context)
        if (not prefix or len(prefix.strip()) == 0) and (not suffix or len(suffix.strip()) == 0):
            skipped += 1
            texts.append("")
            continue

        # Build context section from context_items if available
        context_section = ""
        if context_items and len(context_items) > 0:
            context_section = "### Relevant code context from repository:\n\n"
            for item in context_items[:3]:  # Use top 3 context items
                if item and "content" in item:
                    file_path = item.get("file_path", "unknown")
                    content = item["content"]
                    context_section += f"```{lang}\n// From: {file_path}\n{content}\n```\n\n"

        # Build user message with FIM task
        user_content = f"{context_section}### Complete the following {lang} code:\n\n```{lang}\n{prefix}<FILL_HERE>{suffix}\n```"

        # Create message structure
        messages = [
            {"role": "system", "content": f"You are an expert code completion assistant. Complete the code at the <FILL_HERE> marker with syntactically correct {lang} code that fits naturally with the surrounding context."},
            {"role": "user", "content": user_content},
            {"role": "assistant", "content": middle}
        ]

        # Apply chat template
        try:
            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
            texts.append(text)
        except Exception as e:
            print(f"Warning: Failed to format example {i}: {e}")
            skipped += 1
            texts.append("")

    if skipped > 0:
        print(f"⚠ Skipped {skipped}/{len(examples['prefix'])} examples due to validation issues")

    return {"text": texts}

from datasets import load_dataset

dataset = load_dataset("sourcegraph/context-aware-fim-code-completions", split="train")
dataset

# No need to standardize - this is already in FIM format
dataset = dataset.map(formatting_prompts_func, batched = True,)

# Filter out empty/skipped examples
print(f"\n📊 Dataset size before filtering: {len(dataset)}")
dataset = dataset.filter(lambda x: x['text'] and len(x['text'].strip()) > 0)
print(f"📊 Dataset size after filtering: {len(dataset)}")

# Data inspection - check the first few examples
print("\n=== DATA INSPECTION ===")
print(f"Total examples after filtering: {len(dataset)}")

# Find first valid example
first_valid_idx = 0
for i in range(min(10, len(dataset))):
    if dataset[i]['text'] and len(dataset[i]['text'].strip()) > 0:
        first_valid_idx = i
        break

print(f"\n=== FORMATTED TEMPLATE OUTPUT (Example {first_valid_idx}) ===")
print(dataset[first_valid_idx]['text'][:2000])  # Print first 2000 chars
if len(dataset[first_valid_idx]['text']) > 2000:
    print(f"... [truncated, total length: {len(dataset[first_valid_idx]['text'])} chars]")
print("=== END OF TEMPLATE ===")

# Check a few examples for their middle completion
print("\n=== SAMPLE COMPLETIONS ===")
for i in range(min(3, len(dataset))):
    try:
        # Extract the assistant content from the formatted text
        text = dataset[i]['text']
        if 'assistant' in text:
            # Try to find the completion part
            parts = text.split('assistant')
            if len(parts) > 1:
                completion_snippet = parts[-1][:100].replace('\n', '\\n')
                print(f"Example {i} completion preview: {completion_snippet}...")
    except:
        pass

print("\n=== ANALYZING TEMPLATE STRUCTURE FOR MASKING ===")
sample_text = dataset[first_valid_idx]['text']

# Show a snippet with visible special characters
print("\n=== Template snippet (showing first 500 chars with escape sequences) ===")
snippet = sample_text[:500]
print(repr(snippet))

print("\n=== Searching for masking patterns ===")
# Check various ChatML variations
patterns_found = {}
patterns_to_check = [
    ("<|im_start|>user\n", "ChatML user with newline"),
    ("<|im_start|>user", "ChatML user without newline"),
    ("<|im_start|>assistant\n", "ChatML assistant with newline"),
    ("<|im_start|>assistant", "ChatML assistant without newline"),
    ("<|start|>user<|message|>", "Harmony user"),
    ("<|start|>assistant<|channel|>final<|message|>", "Harmony assistant with channel"),
]

for pattern, desc in patterns_to_check:
    if pattern in sample_text:
        patterns_found[pattern] = desc
        print(f"✓ Found: {desc} → {repr(pattern)}")

if not patterns_found:
    print("✗ No known patterns found! Template format may be unusual.")
    print("  First 200 chars:", repr(sample_text[:200]))

"""Qwen3 uses ChatML format for chat templates, which is simpler than Harmony but very effective for code completion tasks."""

from trl import SFTConfig, SFTTrainer
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    args = SFTConfig(
        per_device_train_batch_size = 1,
        gradient_accumulation_steps = 4,
        warmup_ratio = 0.1,  # Warmup for 10% of training for stability
        num_train_epochs = 1, # Set this for 1 full training run.
        # max_steps = 30,
        learning_rate = 1e-5,  # Further lowered from 5e-5 to prevent NaN loss
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none", # Use TrackIO/WandB etc
        max_grad_norm = 0.5,  # Tighter gradient clipping (was 1.0)
        fp16 = False,  # Disable mixed precision if causing issues
        bf16 = False,
    ),
)

"""We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes and lower loss as well!"""

# ⚙️ MASKING CONFIGURATION
# Set to False to disable masking and train on full sequences (useful for debugging NaN loss)
ENABLE_RESPONSE_MASKING = False  # Set to True once masking patterns are confirmed working

from unsloth.chat_templates import train_on_responses_only

# Auto-detect the correct masking pattern based on the template output
sample_text = dataset[first_valid_idx]['text']

# Try different ChatML variations (with/without newlines)
instruction_part = None
response_part = None

# Priority order: most specific to least specific
patterns_to_try = [
    # ChatML with newlines (most common in Qwen models)
    ("<|im_start|>user\n", "<|im_start|>assistant\n", "ChatML with newlines"),
    # ChatML without newlines
    ("<|im_start|>user", "<|im_start|>assistant", "ChatML without newlines"),
    # Harmony with channels
    ("<|start|>user<|message|>", "<|start|>assistant<|channel|>final<|message|>", "Harmony with channels"),
    # Harmony without channels
    ("<|start|>user<|message|>", "<|start|>assistant<|message|>", "Harmony without channels"),
]

for user_pat, asst_pat, desc in patterns_to_try:
    if user_pat in sample_text and asst_pat in sample_text:
        instruction_part = user_pat
        response_part = asst_pat
        print(f"✓ Using {desc}")
        print(f"  Instruction pattern: {repr(instruction_part)}")
        print(f"  Response pattern: {repr(response_part)}")
        break

if not ENABLE_RESPONSE_MASKING:
    print("🔧 MASKING DISABLED (ENABLE_RESPONSE_MASKING = False)")
    print("   Training on full sequences (user prompts + assistant responses)")
    print("   This helps debug NaN loss issues.")
    print("   Once training works, set ENABLE_RESPONSE_MASKING = True for better quality.")
elif instruction_part is None or response_part is None:
    print("⚠ WARNING: Could not auto-detect masking patterns!")
    print("  Disabling response-only masking to avoid all-zero loss.")
    print("  This will train on both user prompts and assistant responses.")
else:
    # Apply masking
    masking_kwargs = dict(instruction_part=instruction_part, response_part=response_part)
    trainer = train_on_responses_only(
        trainer,
        **masking_kwargs,
    )
    print(f"✅ Masking enabled: Training only on assistant responses")
    print(f"   Instruction masked at: {repr(instruction_part)}")
    print(f"   Response starts at: {repr(response_part)}")

"""Let's verify the training data before starting training."""

# Check a few examples from the training dataset
print("\n=== TRAINING DATA VALIDATION ===")
print(f"Total training examples: {len(trainer.train_dataset)}")

# Check first example
example_idx = 0
try:
    input_ids = trainer.train_dataset[example_idx]["input_ids"]

    print(f"\nExample {example_idx}:")
    print(f"  Input length: {len(input_ids)} tokens")

    # Check if labels exist (they won't if masking is disabled)
    if "labels" in trainer.train_dataset[example_idx]:
        labels = trainer.train_dataset[example_idx]["labels"]
        print(f"  Label length: {len(labels)} tokens")

        # Count non-masked labels
        non_masked_labels = sum(1 for label in labels if label != -100)
        masked_labels = sum(1 for label in labels if label == -100)

        print(f"  Non-masked tokens (will train on): {non_masked_labels}")
        print(f"  Masked tokens (ignored): {masked_labels}")

        if non_masked_labels == 0:
            print("  ⚠️ WARNING: All labels are masked! This will cause NaN loss.")
        elif non_masked_labels < 10:
            print("  ⚠️ WARNING: Very few trainable tokens. Check masking.")
        else:
            print("  ✓ Data looks valid")
    else:
        print("  ℹ️ Labels not yet created (masking disabled)")
        print("  ✓ SFTTrainer will create labels automatically from input_ids")
        print("  ✓ Will train on full sequences")

    # Show decoded text
    print(f"\n  Decoded input (first 200 chars):")
    decoded = tokenizer.decode(input_ids[:200])
    print(f"  {repr(decoded)[:200]}...")

except Exception as e:
    print(f"  ❌ Error checking example: {e}")
    import traceback
    traceback.print_exc()

print("\n" + "="*50)

"""Now let's print the masked out example - you should see only the answer is present:"""

# Only try to decode labels if they exist (when masking is enabled)
if ENABLE_RESPONSE_MASKING and "labels" in trainer.train_dataset[100]:
    decoded_labels = tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100]["labels"]]).replace(tokenizer.pad_token, " ")
    print("Masked labels (what model trains on):", decoded_labels[:200])
else:
    print("Skipping label decode (masking disabled or labels not created)")

# @title Show current memory stats
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

"""Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"""

trainer_stats = trainer.train()

# @title Show final memory and time stats
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory / max_memory * 100, 3)
lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(
    f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training."
)
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")

"""<a name="Inference"></a>
### Inference
Let's test the fine-tuned model on code completion! You can change the code example and programming language.
"""

# Example with context from repository
messages = [
    {"role": "system", "content": "You are an expert code completion assistant. Complete the code at the <FILL_HERE> marker with syntactically correct Python code that fits naturally with the surrounding context."},
    {"role": "user", "content": """### Relevant code context from repository:

```python
// From: utils/validators.py
def is_valid_email(email):
    import re
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'
    return re.match(pattern, email) is not None
```

### Complete the following Python code:

```python
def validate_user_input(data):
    if 'email' not in data:
        raise ValueError("Email is required")
    <FILL_HERE>
```"""},
]
inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
    return_dict = True,
    enable_thinking = False,  # Disable Qwen3's thinking mode for faster inference
).to("cuda")
from transformers import TextStreamer
_ = model.generate(
    **inputs,
    max_new_tokens = 128,
    temperature = 0.7,  # Sampling for faster generation
    do_sample = True,   # Enable sampling
    streamer = TextStreamer(tokenizer)
)

"""<a name="Save"></a>
### Saving, loading finetuned models
To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.

**[NOTE]** Currently finetunes can only be loaded via Unsloth in the meantime - we're working on vLLM and GGUF exporting!
"""

model.save_pretrained("finetuned_model")
# model.push_to_hub("hf_username/finetuned_model", token = "hf_...") # Save to HF

"""To run the finetuned model, you can do the below after setting `if False` to `if True` in a new instance."""

if False:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "finetuned_model", # YOUR MODEL YOU USED FOR TRAINING
        max_seq_length = 1024,
        dtype = None,
        load_in_4bit = True,
    )

messages = [
    {"role": "system", "content": "You are an expert code completion assistant. Complete the code at the <FILL_HERE> marker with syntactically correct TypeScript code that fits naturally with the surrounding context."},
    {"role": "user", "content": """### Complete the following TypeScript code:

```typescript
interface User {
    id: string;
    name: string;
    email: string;
    <FILL_HERE>
}
     
console.log(User.id)
console.log(User.name)
console.log(User.email)
console.log(User.createAt)
```"""},
]
inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
    return_dict = True,
    enable_thinking = False,  # Disable Qwen3's thinking mode for faster inference
).to("cuda")
from transformers import TextStreamer
_ = model.generate(
    **inputs,
    max_new_tokens = 128,
    temperature = 0.7,  # Sampling for faster generation
    do_sample = True,   # Enable sampling
    streamer = TextStreamer(tokenizer)
)

"""### Saving to float16 for VLLM or mxfp4

We also support saving to `float16` or `mxfp4` directly. Select `merged_16bit` for float16. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens.
"""

# Merge and push to hub in mxfp4 4bit format
if False:
    model.save_pretrained_merged("finetuned_model", tokenizer, save_method = "mxfp4")
if False: model.push_to_hub_merged("repo_id/repo_name", tokenizer, token = "hf...", save_method = "mxfp4")

# Merge and push to hub in 16bit
if False:
    model.save_pretrained_merged("finetuned_model", tokenizer, save_method = "merged_16bit")
if False: # Pushing to HF Hub
    model.push_to_hub_merged("hf/gpt-oss-finetune", tokenizer, save_method = "merged_16bit", token = "")

"""And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!

Some other links:
1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)
2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)
3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)
6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!

<div class="align-center">
  <a href="https://unsloth.ai"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
  <a href="https://discord.gg/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord.png" width="145"></a>
  <a href="https://docs.unsloth.ai/"><img src="https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true" width="125"></a>

  Join Discord if you need help + ⭐️ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐️
</div>

  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).

"""
