# Environment Configuration
# All variables use FIM_ prefix followed by section prefix
# Example: FIM_MODEL_NAME, FIM_TRAIN_MAX_STEPS, FIM_LOG_LEVEL

# Model Configuration
FIM_MODEL_NAME=unsloth/Qwen3-8B-bnb-4bit
FIM_MODEL_MAX_SEQ_LENGTH=1024
FIM_MODEL_LOAD_IN_4BIT=true
FIM_MODEL_DTYPE=

# LoRA Configuration
FIM_LORA_R=8
FIM_LORA_ALPHA=16
FIM_LORA_DROPOUT=0.0

# Data Configuration
FIM_DATA_DATASET_NAME=sourcegraph/context-aware-fim-code-completions
FIM_DATA_SPLIT=train
FIM_DATA_MAX_CONTEXT_ITEMS=3
FIM_DATA_ENABLE_RESPONSE_MASKING=false

# Training Configuration
FIM_TRAIN_OUTPUT_DIR=outputs
FIM_TRAIN_NUM_TRAIN_EPOCHS=1
# FIM_TRAIN_MAX_STEPS=10  # Use for minimal testing (overrides num_train_epochs if set)
FIM_TRAIN_PER_DEVICE_TRAIN_BATCH_SIZE=1
FIM_TRAIN_GRADIENT_ACCUMULATION_STEPS=4
FIM_TRAIN_LEARNING_RATE=1e-5
FIM_TRAIN_WARMUP_RATIO=0.1
FIM_TRAIN_MAX_GRAD_NORM=0.5
FIM_TRAIN_LOGGING_STEPS=10
FIM_TRAIN_SAVE_STEPS=500
FIM_TRAIN_OPTIM=adamw_8bit
FIM_TRAIN_WEIGHT_DECAY=0.01
FIM_TRAIN_LR_SCHEDULER_TYPE=linear
FIM_TRAIN_SEED=3407
FIM_TRAIN_FP16=false
FIM_TRAIN_BF16=false

# Inference Configuration
FIM_INFER_MAX_NEW_TOKENS=128
FIM_INFER_TEMPERATURE=0.7
FIM_INFER_DO_SAMPLE=true
FIM_INFER_ENABLE_THINKING=false

# Logging
FIM_LOG_LEVEL=INFO
